# Human Evaluation Template Specification v1.0

**Document Version**: v1.0  
**Date**: June 7, 2025  
**Purpose**: Specification for spreadsheet template used in human evaluation of AI agent outputs

---

## üìä Template Overview

### Format
- **Primary Tool**: Google Sheets or Excel spreadsheet
- **Import Format**: CSV for batch data loading
- **Evaluation Method**: Manual scoring with structured rubrics
- **Output**: Quantitative scores (1-5) and qualitative comments

### Template Structure
The evaluation template contains the following column categories:
1. **Identification Columns**: Unique identifiers and linking information
2. **Context Columns**: Input summaries and agent information  
3. **Output Columns**: Agent-generated content to be evaluated
4. **Metric Columns**: Score and comment pairs for each human metric
5. **Overall Assessment**: Holistic quality evaluation
6. **Administrative Columns**: Evaluator tracking and timestamps

---

## üìã Column Definitions

### Core Identification Columns

| Column Name | Type | Description | Example |
|-------------|------|-------------|---------|
| `eval_id` | String | Unique identifier for this evaluation entry | `EVAL_001_PRD_MOBILE` |
| `golden_dataset_inputId` | String | Links back to golden_datasets_v1.json | `prd_simple_001` |
| `case_id` | String | Business case ID from agent run (if applicable) | `case_12345` |
| `trace_id` | String | Trace ID from agent logs for this run | `trace_abc123def456` |

### Context & Content Columns

| Column Name | Type | Description | Example |
|-------------|------|-------------|---------|
| `agent_name` | String | Name of the agent being evaluated | `ProductManagerAgent` |
| `input_payload_summary` | Text | Key parts of the input shown to agent | `Patient Portal Mobile App - Secure access to medical records...` |
| `agent_output_to_evaluate` | Text | Complete output generated by agent | `# Patient Portal Mobile App PRD\n\n## Introduction\n...` |

### Human Metric Columns (Agent-Specific)

#### ProductManagerAgent Metrics

| Column Name | Type | Description | Range |
|-------------|------|-------------|-------|
| `Content_Relevance_Quality_Score` | Integer | Score for content relevance and quality | 1-5 |
| `Content_Relevance_Quality_Comment` | Text | Detailed feedback on content quality | Free text |

#### ArchitectAgent Metrics  

| Column Name | Type | Description | Range |
|-------------|------|-------------|-------|
| `Plausibility_Appropriateness_Score` | Integer | Score for architecture plausibility | 1-5 |
| `Plausibility_Appropriateness_Comment` | Text | Feedback on architecture appropriateness | Free text |
| `Clarity_Understandability_Score` | Integer | Score for design clarity | 1-5 |
| `Clarity_Understandability_Comment` | Text | Feedback on documentation clarity | Free text |

#### PlannerAgent Metrics

| Column Name | Type | Description | Range |
|-------------|------|-------------|-------|
| `Reasonableness_Hours_Score` | Integer | Score for hour estimate reasonableness | 1-5 |
| `Reasonableness_Hours_Comment` | Text | Feedback on effort estimates | Free text |
| `Quality_Rationale_Score` | Integer | Score for estimation rationale quality | 1-5 |
| `Quality_Rationale_Comment` | Text | Feedback on reasoning quality | Free text |

#### SalesValueAnalystAgent Metrics

| Column Name | Type | Description | Range |
|-------------|------|-------------|-------|
| `Plausibility_Projections_Score` | Integer | Score for value projection plausibility | 1-5 |
| `Plausibility_Projections_Comment` | Text | Feedback on value projections | Free text |

### Overall Assessment Columns

| Column Name | Type | Description | Range |
|-------------|------|-------------|-------|
| `overall_quality_score` | Integer | Holistic assessment of agent output | 1-5 |
| `overall_comments` | Text | General feedback on this agent run | Free text |

### Administrative Columns

| Column Name | Type | Description | Example |
|-------------|------|-------------|---------|
| `evaluator_id` | String | Identifier for the human evaluator | `evaluator_001` |
| `evaluation_date` | Date | Date when evaluation was completed | `2025-06-07` |

---

## üìù CSV Header Template

### Complete Header Row
```csv
eval_id,golden_dataset_inputId,case_id,trace_id,agent_name,input_payload_summary,agent_output_to_evaluate,Content_Relevance_Quality_Score,Content_Relevance_Quality_Comment,Plausibility_Appropriateness_Score,Plausibility_Appropriateness_Comment,Clarity_Understandability_Score,Clarity_Understandability_Comment,Reasonableness_Hours_Score,Reasonableness_Hours_Comment,Quality_Rationale_Score,Quality_Rationale_Comment,Plausibility_Projections_Score,Plausibility_Projections_Comment,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

### Minimal Header (Core Columns Only)
```csv
eval_id,golden_dataset_inputId,agent_name,input_payload_summary,agent_output_to_evaluate,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

---

## üîß Template Usage Instructions

### Setting Up the Spreadsheet

1. **Create New Spreadsheet**: Google Sheets or Excel
2. **Add Header Row**: Copy appropriate CSV header based on evaluation scope
3. **Format Columns**: 
   - Score columns: Number format, validation (1-5 range)
   - Text columns: Text wrapping enabled
   - Date columns: Date format
4. **Apply Data Validation**: Restrict score columns to 1-5 integer values
5. **Conditional Formatting**: Color-code scores (red=1, yellow=3, green=5)

### Data Import Process

1. **Prepare CSV Data**: Use `human_eval_batch_01_inputs_outputs.json` to generate CSV
2. **Import to Spreadsheet**: File ‚Üí Import ‚Üí CSV
3. **Verify Data**: Check that all columns align correctly
4. **Apply Formatting**: Ensure score validation and text wrapping

### Evaluation Workflow

1. **Review Guidelines**: Familiarize with `human_evaluation_guidelines_v1.md`
2. **Read Input Context**: Understand the original problem and expected output
3. **Evaluate Agent Output**: Apply rubrics systematically
4. **Score Each Metric**: Use 1-5 scale consistently
5. **Add Comments**: Provide specific, constructive feedback
6. **Overall Assessment**: Holistic quality score and general comments

---

## üìä Template Variations

### Agent-Specific Templates
For focused evaluations, create agent-specific templates with only relevant metric columns:

#### ProductManagerAgent Template
```csv
eval_id,golden_dataset_inputId,agent_name,input_payload_summary,agent_output_to_evaluate,Content_Relevance_Quality_Score,Content_Relevance_Quality_Comment,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

#### ArchitectAgent Template  
```csv
eval_id,golden_dataset_inputId,agent_name,input_payload_summary,agent_output_to_evaluate,Plausibility_Appropriateness_Score,Plausibility_Appropriateness_Comment,Clarity_Understandability_Score,Clarity_Understandability_Comment,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

#### PlannerAgent Template
```csv
eval_id,golden_dataset_inputId,agent_name,input_payload_summary,agent_output_to_evaluate,Reasonableness_Hours_Score,Reasonableness_Hours_Comment,Quality_Rationale_Score,Quality_Rationale_Comment,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

#### SalesValueAnalystAgent Template
```csv
eval_id,golden_dataset_inputId,agent_name,input_payload_summary,agent_output_to_evaluate,Plausibility_Projections_Score,Plausibility_Projections_Comment,overall_quality_score,overall_comments,evaluator_id,evaluation_date
```

---

## üéØ Quality Assurance Features

### Data Validation Rules
- **Score Columns**: Integer values 1-5 only
- **Required Fields**: eval_id, agent_name, overall_quality_score, evaluator_id
- **Date Format**: YYYY-MM-DD format for evaluation_date

### Conditional Formatting
- **Score 1-2**: Red background (Poor quality)
- **Score 3**: Yellow background (Average quality)  
- **Score 4-5**: Green background (Good to excellent quality)

### Formula Suggestions
- **Average Score**: Calculate average across applicable metrics for each row
- **Score Distribution**: Summary statistics for evaluator calibration
- **Comment Length**: Track comment completeness and detail

---

## üìà Analysis Capabilities

### Quantitative Analysis
- Score distributions by agent and metric
- Inter-evaluator reliability statistics
- Performance trends across different input types
- Correlation between metrics and overall quality

### Qualitative Analysis  
- Comment categorization and themes
- Specific improvement recommendations
- Examples of excellent vs. poor outputs
- Evaluator feedback patterns

---

## üîÑ Template Evolution

### Version Control
- Track template version in spreadsheet metadata
- Document changes between versions
- Maintain backward compatibility where possible

### Feedback Integration
- Collect evaluator feedback on template usability
- Iteratively improve column organization and validation
- Add new metrics based on evaluation insights

---

**Template Designed By**: AI/ML Evaluation Process Designer  
**Date**: June 7, 2025  
**Status**: Ready for implementation and testing 